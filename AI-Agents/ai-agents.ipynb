{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agents for ERP Applications\n",
    "\n",
    "Transforming ERP with AI Agents: Advanced Fraud Detection, Waste management, Anomaly detection and Compliance\n",
    "\n",
    "```\n",
    "Author:     Amit Shukla\n",
    "contact:    X.com/@ashuklax \n",
    "            youtube.com/@amit.shukla\n",
    "```\n",
    "- Video:      [link](https://youtube.com/@amit.shukla)\n",
    "- Blog:       [link](https://x.com/@ashuklax/highlights)\n",
    "\n",
    "**Table of Contents**\n",
    "- Objective\n",
    "- Demo\n",
    "- Prerequisites for basic setup\n",
    "- Pro App\n",
    "- Learning path\n",
    "- Choosing an AI Agent framework\n",
    "- Solution design\n",
    "- Implementation\n",
    "    - AI Agents - AutoGen Studio\n",
    "    - AI Agents - AutoGen AgentChat\n",
    "    - AI Agents - AutoGen Magentic-One\n",
    "    - AI Agents - AutoGen Core\n",
    "    - AI Agents - AutoGen Extension\n",
    "\n",
    "## Background\n",
    "Thank you for joining me in this blog series! This series builds on the [Vision OCR AI](https://www.youtube.com/playlist?list=PLp0TENYyY8lFwTOS7KHZ3scBM0j5C6TXe) we developed earlier. \n",
    "\n",
    "Here, we aim to create Gen AI Agent-based systems for large ERP platforms, progressing step by step. In this blog, we’ll focus on the Vision AI OCR Agent framework. While it might not seem like a typical ERP AI Agent use case now, it’s the foundation for solving bigger challenges.\n",
    "\n",
    "Once this basic OCR Vision AI app is complete, we’ll expand the framework to handle more complex ERP tasks, such as processing invoices and matching them with receipts and payments. This foundation we will build in this series will support many future use cases.\n",
    "\n",
    "## Objective\n",
    "In this blog series, we'll build an AI-Agent driven ERP solution that's intelligent, autonomous, scalable, secure and capable of supporting hundreds of thousands of employees, managing millions of documents, and handling petabytes of data to serve ERP systems.\n",
    "\n",
    "## Demo\n",
    "[link](https://youtube.com/@amit.shukla)\n",
    "\n",
    "This snapshot shows the Teams, Agents, Tools, and models behind our Vision AI OCR process. \n",
    "\n",
    "Agent Studio, is a web-based interface, offers an easy-to-understand visual representation of our work and is simple to set up. \n",
    "\n",
    "As we move toward building a more advanced ERP solution, we’ll need to dive deeper, using AutoGen Agent and AutoGen Core to leverage full event-driven programming and distribute AI workloads across a multi-agent framework. \n",
    "\n",
    "For now, since our app is straightforward, Agent Studio and Agent Chat meet our needs.\n",
    "\n",
    "## Prerequisites for basic setup\n",
    "This is an intermediate-level series, so you'll need a basic understanding of ERP systems, Generative AI, Machine Learning, LLM models, Python, SQL, vector databases, PyTorch, and Transformers.\n",
    "If you are a beginner, just watch this at slow pace and still this series will have a lot to offer you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pro App development\n",
    "\n",
    "Building Pro ERP and SaaS applicaiton require extensive business process evaluation, comprehensive planning, solution design, technical architecture, and implementation. \n",
    "\n",
    "connect [x.com/@ashuklax](x.com/@ashuklax)\n",
    "\n",
    "`use cases`\n",
    "- Help Desk\n",
    "- T & E\n",
    "    - auto expense creation\n",
    "    - auto auditor meter\n",
    "    - pay faster\n",
    "    - expense checker\n",
    "    - vendor discount finder\n",
    "    - employee discount finder\n",
    "    - personal AI Agent, AI Assistant\n",
    "- Fraud, Alert, Waste, Compliance\n",
    "- Auto Responder\n",
    "- Duplicate Invoice, Expense, Voucher, Receipt Finder\n",
    "    - Fraud Detection\n",
    "    - Automate Document sorting, de-duplication\n",
    "    - Automate data reconciliation\n",
    "    - Audit 3 way document match to find exceptions (request, pay, voucher, invoice, receipt)\n",
    "- Inventory\n",
    "    - cycle count\n",
    "    - replenishment\n",
    "    - auto order\n",
    "    - auto receipts\n",
    "    - anaomaly detection\n",
    "    - usage warning\n",
    "    - theft prevention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning path\n",
    "It's a code-along series where we'll create the entire solution from scratch to automate ERP processes.\n",
    "\n",
    "I also have two beginner-friendly series for those who want to learn the fundamentals of Generative AI and how to create a simple AI app. If you're looking for beginner-friendly tutorials, be sure to check out those videos!\n",
    "\n",
    "- [Gen AI Apps - OCR Vision AI - open source](https://www.youtube.com/playlist?list=PLp0TENYyY8lFwTOS7KHZ3scBM0j5C6TXe)\n",
    "- [Gen AI RAG basics - Host, build, serve app](https://www.youtube.com/playlist?list=PLp0TENYyY8lF8EsgtfDoPkuAgxc-lcwbd)\n",
    "\n",
    "Here are the AI Agents we will build in this series\n",
    "\n",
    "- AI Agents - AutoGen Studio\n",
    "- AI Agents - AutoGen AgentChat\n",
    "- AI Agents - AutoGen Magentic-One\n",
    "- AI Agents - AutoGen Core\n",
    "- AI Agents - AutoGen Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Choose the Right AI Agent Framework\n",
    "\n",
    "For small tasks, use of tools and functions on llama or Open AI models are enough.\n",
    "\n",
    "Then, once you get into a stage, when you have many many tools and functions to manage, in that case, using an AI Agent Framework like LlamaIndex, LangChain, CrewAI, AutoGen, or Swarms are often required.\n",
    "\n",
    "To decide, spend no more than an hour reading the quick start guides for CrewAI, AutoGen, and LangChain.\n",
    "These frameworks are all easy to begin with and all it takes is to read their Quick Start Guide.\n",
    "\n",
    "Here’s a trick: after reading each AI Agent framework's Quick Start Guide, choose the one that makes you want to open your code editor and start coding.\n",
    "That’s your best choice—don’t overthink it.\n",
    "The programming language, framework or any logic which pushes you to open code editor is the way you want to go.\n",
    "\n",
    "You might question your decision later, but that’s part of being a developer. You learn by doing, so avoid wasting time on videos or opinions. Just pick one and start coding; you’ll figure out the right fit within a few hours.\n",
    "\n",
    "For my business use case, supporting a high-performing ERP system with significant complexity, data volume, and frequency, \n",
    "I chose AutoGen. Since my ERP runs on Azure, starting with AutoGen felt easier to support within the same cloud environment. \n",
    "\n",
    "If I later feel LangChain or CrewAI would’ve been a better choice, I’m happy to refactor. It’s all part of the process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "stateDiagram-v2\n",
    "        direction LRstateDiagram-v2\n",
    "        [*] --> User_Query\n",
    "        User_Query --> Conversation_AI_Agent\n",
    "        Conversation_AI_Agent --> SQL_DB\n",
    "        SQL_DB --> Conversation_AI_Agent\n",
    "        Conversation_AI_Agent --> RAG_VectorDB\n",
    "        RAG_VectorDB --> Conversation_AI_Agent\n",
    "        [*] --> File_Drop\n",
    "        File_Drop --> PyPDF\n",
    "        File_Drop --> Tesseract\n",
    "        File_Drop --> AzureDocementService\n",
    "        File_Drop --> OracleVisionAI\n",
    "        File_Drop --> OtherVisionAPI\n",
    "        PyPDF --> QC\n",
    "        Tesseract --> QC\n",
    "        AzureDocementService --> QC\n",
    "        OracleVisionAI --> QC\n",
    "        OtherVisionAPI --> QC\n",
    "        QC --> QC_AI_Agent\n",
    "        QC_AI_Agent --> QC\n",
    "        QC_AI_Agent --> RAG_VectorDB\n",
    "        QC_AI_Agent --> SQL_DB\n",
    "        QC_AI_Agent --> [*]\n",
    "        Conversation_AI_Agent --> [*]\n",
    "\n",
    "%% Define classes for coloring\n",
    "    classDef red fill:#ff8,stroke:#333,stroke-width:2px;\n",
    "    classDef green fill:#8fa,stroke:#333,stroke-width:2px;\n",
    "    classDef blue fill:#8af,stroke:#333,stroke-width:2px;\n",
    "    classDef orange fill:#f92,stroke:#333,stroke-width:2px;\n",
    "    classDef brown fill:#e6f,stroke:#333,stroke-width:2px;\n",
    "    classDef neil fill:#1ff,stroke:#333,stroke-width:2px;\n",
    "\n",
    "    %% Apply classes to states\n",
    "    class User_Query green\n",
    "    class Conversation_AI_Agent orange\n",
    "    class social green\n",
    "    class Tesseract brown\n",
    "    class SQL_DB blue\n",
    "    class File_Drop green\n",
    "    class RAG_VectorDB blue\n",
    "    class PyPDF brown\n",
    "    class QC_AI_Agent orange\n",
    "    class QC red\n",
    "    class AzureDocementService brown\n",
    "    class OracleVisionAI brown\n",
    "    class OtherVisionAPI brown\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[File Drop] --> B[Dir_Receipts]\n",
    "    A[File Drop] --> C[Dir_Invoice]\n",
    "    A[File Drop] --> D[Dir_Expense]\n",
    "    A[File Drop] --> E[Dir_Misc]\n",
    "    B[Dir_Receipts] --> F[File]\n",
    "    C[Dir_Invoice] --> F[File]\n",
    "    D[Dir_Expense] --> F[File]\n",
    "    E[Dir_Misc] --> F[File]\n",
    "    F[File] --> G{QC_GenAIChain}\n",
    "    G{QC_GenAI_Chain} -->|PyPDF| I[OCR_+_metadata]\n",
    "    G{QC_GenAI_Chain} -->|Tesseract| I[OCR_+_metadata]\n",
    "    G{QC_GenAI_Chain} -->|llama vision| I[OCR_+_metadata]\n",
    "    I[OCR_+_metadata] --> J[SQLDB]\n",
    "    I[OCR_+_metadata] --> K[VectorDB]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processes\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[File Drop] --> |Agent_FileDrop| F[File]\n",
    "    F[File] --> |Agent_QC| G{QC_GenAIChain}\n",
    "    G{QC_GenAI_Chain} -->|Agent_PyPDF| I[OCR_+_metadata]\n",
    "    G{QC_GenAI_Chain} -->|Agent_Tesseract| I[OCR_+_metadata]\n",
    "    G{QC_GenAI_Chain} -->|Agent_llama vision| I[OCR_+_metadata]\n",
    "    I[OCR_+_metadata] --> |Agent_SQLDB| J[SQLDB]\n",
    "    I[OCR_+_metadata] --> |Agent_VectorDB| K[VectorDB]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Design Pattern\n",
    "\n",
    "```mermaid\n",
    "\n",
    "graph TD;\n",
    "    Tool_Filer-->Llama3.2-->Agent_Filer-->Team_Filer;\n",
    "    Team_Filer-->Team_OCR;\n",
    "    Tool_PDF-->PyPDF-->Agent_OCR-->Team_OCR;\n",
    "    Tool_Tesseract-->Tesseract-->Agent_OCR-->Team_OCR;\n",
    "    Tool_Vision-->Vision_AI-->Agent_OCR-->Team_OCR;\n",
    "    Tool_SQL-->SQLDB-->Agent_DB-->Team_DB;\n",
    "    Tool_VecDB-->VectorDB-->Agent_DB-->Team_DB;\n",
    "    Team_OCR-->Team_DB;\n",
    "\n",
    "    style Tool_Filer fill:#68e,stroke:#333,stroke-width:2px;\n",
    "    style Tool_PDF fill:#68e,stroke:#333,stroke-width:2px;\n",
    "    style Tool_Tesseract fill:#68e,stroke:#333,stroke-width:2px;\n",
    "    style Tool_Vision fill:#68e,stroke:#333,stroke-width:2px;\n",
    "    style Tool_SQL fill:#68e,stroke:#333,stroke-width:2px;\n",
    "    style Tool_VecDB fill:#68e,stroke:#333,stroke-width:2px;\n",
    "\n",
    "    style Agent_Filer fill:#666,stroke:#333,stroke-width:2px;\n",
    "    style Agent_OCR fill:#666,stroke:#333,stroke-width:2px;\n",
    "    style Agent_DB fill:#666,stroke:#333,stroke-width:2px;\n",
    "\n",
    "    style Team_Filer fill:#a53,stroke:#333,stroke-width:2px;\n",
    "    style Team_OCR fill:#a53,stroke:#333,stroke-width:2px;\n",
    "    style Team_DB fill:#a53,stroke:#333,stroke-width:2px;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Filer: \n",
    "Team Filer's job is to monitor images and documents uploaded by users through a ChatBot or web app. They also handle documents from automated batch schedules, which send and receive files in various formats like Word, PDF, GIF, PNG, and JPEG. These files are usually placed in assigned directories or shared network locations.\n",
    "\n",
    "Team Filer job is nothing but to monitor images, documents loaded by users through a ChatBot interface or any other web app, along with all other documents received by hundreds of thousands of automated batch schedules which send and receive files in different formats such as word, pdf documents and images in different format such as gif, png and jpegs.\n",
    "These files are ideally dropped into assigned directories or shared network places.\n",
    "\n",
    "Team Filer's job is to monitor each file, collect its metadata, and process it. Once processed, they rename or mark the file as \"processed.\" If there's an error, they report it in the error table and move on to the next file.\n",
    "\n",
    "`Metadata`\n",
    "\n",
    "- File Type\n",
    "- File Owner\n",
    "- File Size\n",
    "- misc..\n",
    "\n",
    "**why Team Filer is a separate team?**\n",
    "Team Filer is separate because our long-term goal is to distribute it. ERP systems are large and often use multiple systems to send and receive files. Keeping Team Filer separate from the start allows us to provide them with specialized tools and infrastructure for scaling when needed to support distributed computing.\n",
    "\n",
    "Team Filer will have one tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team OCR:\n",
    "Team OCR's job is extract text from given PDF, image (png, gif or jpeg) or use vision models to read text.\n",
    "\n",
    "`tasks`\n",
    "\n",
    "- Extract texts from PDF using PyPDF\n",
    "- Extract texts from image using Tesseract\n",
    "- Extract texts from image using LLM Vision models\n",
    "\n",
    "<!-- Team OCR will ideally have three agents that work `sequentially` and stop once high-quality OCR content is extracted from the document. -->\n",
    "Team OCR will ideally have one agent with 3 tools working `sequentially` and stop once high-quality OCR content is extracted from the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team DB: \n",
    "Team Database's job is to insert data into two different database each used for different purpose.\n",
    "\n",
    "`tasks`\n",
    "\n",
    "- SQL Database for structured data\n",
    "- Vector Database for un-structured data stored in embedding tokens\n",
    "- file storage\n",
    "\n",
    "<!-- Team DB ideally will have two Agents, which can work in `parallel`. -->\n",
    "Team DB ideally will have one Agent, with two tools.\n",
    "\n",
    "<!-- ## Definitions\n",
    "### Agent_FileDrop\n",
    "### Agent_PyPDF\n",
    "### Agent_Tesseract\n",
    "### Agent_llamaVision\n",
    "### Agent_SQLDB\n",
    "### Agent_VectorDB\n",
    "### Agent_QC -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern: Team Filer\n",
    "\n",
    "- Step 0: SBAR\n",
    "- Step 1: Getting Started | Installation\n",
    "- Step 2: Write Tool\n",
    "- Step 3: Write Agent\n",
    "- Step 4: Build Team\n",
    "- Step 5: use Models\n",
    "- Step 6: Unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: SBAR\n",
    "Situation, Background, Assessment and Recommendation\n",
    "\n",
    "#### Situation\n",
    "We are here to build one TEAM Team Filer, it's job is to monitor any File Drop at Directory.\n",
    "As soon as file is dropped, need to read File MetaData content (see below defn of Metadata) and then, once Metadata is read,\n",
    "rename the file or mark it indicating that this file is already processed.\n",
    "\n",
    "`Metadata`\n",
    "\n",
    "- File Path\n",
    "- File Type\n",
    "- File Owner\n",
    "- File Size\n",
    "- misc..\n",
    "\n",
    "#### Background\n",
    "in past, we did acheive this task using 3 scripts (see code block below).\n",
    "These scripts will come handy and some parts will re-used.\n",
    "\n",
    "- Step 1: build a CRON JOB which calls a bash/shell script\n",
    "- Step 2: build a shell script which calls a python script\n",
    "- Step 3: Python script, reads and print file Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code used in past\n",
    "\n",
    "#################################\n",
    "## open CRON Table for editing ##\n",
    "#################################\n",
    "# crontab -e\n",
    "\n",
    "## update CRON Table to execute a shell script `monitor_drive.sh`\n",
    "# * * * * * /path/to/monitor_drive.sh\n",
    "\n",
    "#################################\n",
    "## update bash shell script\n",
    "#################################\n",
    "#!/bin/bash\n",
    "\n",
    "# WATCHED_DIR=\"/path/to/watched/directory\"\n",
    "# LAST_FILE=\"\"\n",
    "\n",
    "# while true; do\n",
    "#     NEW_FILE=$(ls -t \"$WATCHED_DIR\" | head -n 1)\n",
    "#     if [ \"$NEW_FILE\" != \"$LAST_FILE\" ]; then\n",
    "#         LAST_FILE=\"$NEW_FILE\"\n",
    "#         python3 /path/to/file_processor.py\n",
    "#     fi\n",
    "#     sleep 10\n",
    "# done\n",
    "\n",
    "####################################\n",
    "## python script to capture Metadata\n",
    "####################################\n",
    "\n",
    "import os\n",
    "import pwd\n",
    "import mimetypes\n",
    "\n",
    "def get_file_metadata(file_path):\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    \n",
    "    # Get file owner\n",
    "    file_owner = pwd.getpwuid(os.stat(file_path).st_uid).pw_name\n",
    "    \n",
    "    # Get file type\n",
    "    file_type, _ = mimetypes.guess_type(file_path)\n",
    "    \n",
    "    return file_owner, file_size, file_type\n",
    "\n",
    "def print_file_metadata(file_path):\n",
    "    file_owner, file_size, file_type = get_file_metadata(file_path)\n",
    "    print(f\"File Owner: {file_owner}\")\n",
    "    print(f\"File Size: {file_size} bytes\")\n",
    "    print(f\"File Type: {file_type}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = 'example.txt'  # Replace with your file path\n",
    "print_file_metadata(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment\n",
    "Previously used Python script for reading metadata can be reused. There's no need to create a CRON job or write a bash script, as drive monitoring can now be handled using Team Run.\n",
    "\n",
    "#### Recommendation\n",
    "This is easy to build in Agent Chat or Agent Studio, and easy to unit test.\n",
    "\n",
    "Write code in this order. build Tool function, Agent, model, Termination condition, then put it all together in a team to run as stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Getting Started | Installation\n",
    "\n",
    "- setup new Python environment\n",
    "- setup autogen\n",
    "- setup ollama\n",
    "- setup deepseek-r1, llama 3.3 models\n",
    "- setup llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.8\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Create and activate a new Python virtual environment\n",
    "######################################################\n",
    "# Python 3.10 or later is required.\n",
    "\n",
    "!python3 -m venv .venv\n",
    "!source .venv/bin/activate\n",
    "\n",
    "## To deactivate later, run:\n",
    "!deactivate\n",
    "\n",
    "######################################################\n",
    "# install autogen in this section below\n",
    "# only autogen studio and AgentChat is required\n",
    "# AutoGen Core is required in later phase\n",
    "######################################################\n",
    "# install autogen studio\n",
    "! pip install -U autogenstudio\n",
    "# install autogen chat\n",
    "! pip install -U \"autogen-agentchat\"\n",
    "# install autogen OpenAI for Model Client\n",
    "# !pip install \"autogen-ext[openai]\"\n",
    "# install autogen core\n",
    "# !pip install \"autogen-core\"\n",
    "\n",
    "######################################################\n",
    "# run AutoGen Studio GUI\n",
    "######################################################\n",
    "!autogenstudio ui --host <host> --port 8000\n",
    "!autogenstudio ui --port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# install llama.cpp to run deepseek-r1 model\n",
    "#############################################\n",
    "\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd llama.cpp\n",
    "!make\n",
    "\n",
    "# Download the deepseek-r1 Model weights \n",
    "\n",
    "# Convert the Model (if necessary)\n",
    "# If the model weights are not already in GGML format, \n",
    "# you must convert them using the conversion tools provided in llama.cpp. \n",
    "# Assuming the model weights are in PyTorch format:\n",
    "!python3 convert-pth-to-ggml.py path/to/deepseek-r1.pth\n",
    "\n",
    "# Prepare the Model File\n",
    "# Place the .bin model file (e.g., deepseek-r1.ggml.bin) into the llama.cpp directory \n",
    "# or specify its location when running commands\n",
    "\n",
    "# run the model\n",
    "!./main -m ./deepseek-r1.ggml.bin -p \"Your prompt here\"\n",
    "\n",
    "# adjust parameters\n",
    "!./main -m ./deepseek-r1.ggml.bin -p \"Explain quantum mechanics.\" -t 8 -n 128 --temp 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# install ollama to run deepseek-r1 model\n",
    "##########################################\n",
    "\n",
    "! curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "!ollama list\n",
    "!ollama pull deepseek-r1:7b\n",
    "!ollama list\n",
    "!ollama run deepseek-r1\n",
    "\n",
    "# please see, ollama by default serves at 0.0.0.0:11434\n",
    "# you can change this\n",
    "!sudo systemctl edit ollama.service\n",
    "#[Service]\n",
    "Environment=\"OLLAMA_HOST=0.0.0.0:8000\"\n",
    "\n",
    "!sudo systemctl daemon-reload\n",
    "!sudo systemctl restart ollama.service\n",
    "\n",
    "# also, for a quick one time change you can run ollama as\n",
    "!export OLLAMA_HOST=127.0.0.1:8000\n",
    "!ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Write Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this is the standard way of writing ollama tool\n",
    "# ollama is in active dev phase with AutoGen\n",
    "# this code will be refactored later\n",
    "# when AutoGen supports ollama tools\n",
    "# for now, just learn how to write tool functions\n",
    "# and will learn about workaround in later section\n",
    "\n",
    "import os\n",
    "import pwd\n",
    "import mimetypes\n",
    "import shutil\n",
    "\n",
    "file_path_processed = '/processed/dir'\n",
    "# Define a tool\n",
    "async def get_FileMetaData(file_path: str) -> str:\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # Get file owner\n",
    "    file_owner = pwd.getpwuid(os.stat(file_path).st_uid).pw_name\n",
    "    # Get file type\n",
    "    file_type, _ = mimetypes.guess_type(file_path)\n",
    "\n",
    "    # not file metadata is read, move or rename this file as processed\n",
    "    shutil.move(file_path, file_path_processed)\n",
    "    \n",
    "    return \"The file path is {file_path}, file owner is {file_owner}, file size is {file_size} and file type is {file_type}.\"\n",
    "\n",
    "tools = [{\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'get_FileMetaData',\n",
    "        'description': 'Get the current file metadata',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'filePath': {\n",
    "              'type': 'string',\n",
    "              'description': 'file path',\n",
    "            },\n",
    "          },\n",
    "          'required': ['filePath'],\n",
    "        },\n",
    "      },\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autogen\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "async def main() -> None:\n",
    "    # Define an agent: Agent_Filer\n",
    "    weather_agent = AssistantAgent(\n",
    "        name=\"Agent_Filer\",\n",
    "        model_client=OpenAIChatCompletionClient(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            # api_key=\"YOUR_API_KEY\",\n",
    "        ),\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: use local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and run ollama models\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!ollama pull deepseek-r1:7b\n",
    "!ollama list\n",
    "!ollama run deepseek-r1 # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiteLLM is an open-source locally run proxy server that provides an OpenAI-compatible API.\n",
    "# liteLLM will us connect ollama to OpenAI-compatible API\n",
    "# make sure you already ollama installed and running\n",
    "\n",
    "!pip install 'litellm[proxy]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m5151\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\n",
      "\u001b[1;37m#------------------------------------------------------------#\u001b[0m\n",
      "\u001b[1;37m#                                                            #\u001b[0m\n",
      "\u001b[1;37m#               'A feature I really want is...'               #\u001b[0m\n",
      "\u001b[1;37m#        https://github.com/BerriAI/litellm/issues/new        #\u001b[0m\n",
      "\u001b[1;37m#                                                            #\u001b[0m\n",
      "\u001b[1;37m#------------------------------------------------------------#\u001b[0m\n",
      "\n",
      " Thank you for using LiteLLM! - Krrish & Ishaan\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:4000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:49410 - \"\u001b[1mPOST /chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:49410 - \"\u001b[1mPOST /chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:49410 - \"\u001b[1mPOST /chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:42722 - \"\u001b[1mPOST /chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:42722 - \"\u001b[1mPOST /chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:42722 - \"\u001b[1mPOST /chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:46278 - \"\u001b[1mPOST /chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:46278 - \"\u001b[1mPOST /chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:46278 - \"\u001b[1mPOST /chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# run litellm\n",
    "# !litellm --model ollama/deepseek-r1\n",
    "!litellm --model ollama/llama3.2\n",
    "# !ollama pull llama3.2\n",
    "# !ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autogen\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "import asyncio\n",
    "from autogen_core.tools import FunctionTool\n",
    "from autogen_core.models import ModelFamily\n",
    "\n",
    "tools = FunctionTool(get_FileMetaData, description=\"Get the current file metadata\")\n",
    "\n",
    "# update main function to \n",
    "# use locally hosted ollama models\n",
    "##### WARNING ############################\n",
    "# This is an area of active development \n",
    "# and a native Ollama client for AutoGen\n",
    "# is planned for a future release.\n",
    "# workaround is to use litellm\n",
    "##########################################\n",
    "\n",
    "custom_model_client = OpenAIChatCompletionClient(\n",
    "    model=\"deepseek-r1\",\n",
    "    base_url=\"http://0.0.0.0:4000\", # most of other params are unnecessary if litellm is accessed\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False,\n",
    "        # \"family\": ModelFamily.R1,\n",
    "    },\n",
    ")\n",
    "\n",
    "async def main() -> None:\n",
    "    # Define an agent\n",
    "    Agent_Filer = AssistantAgent(\n",
    "                    name=\"Agent_Filer\",\n",
    "                    model_client=custom_model_client,\n",
    "                    # tools=[tools],\n",
    "    )\n",
    "\n",
    "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
    "# await main()\n",
    "asyncio.run(main())\n",
    "\n",
    "# copy this entire code into a python file main.py and run following from a terminal\n",
    "# python main.py\n",
    "# this code will work fine and will accept and answer prompts\n",
    "# however, you will notice that I have commented ~tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ollama tools with AutoGen is in dev phase\n",
    "# so we will use a work around to make it work for now\n",
    "\n",
    "!pip install autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ollama tools with AutoGen is in dev phase\n",
    "# so we will use a work around to make it work for now\n",
    "# This code is good as of Feb 08, 2025\n",
    "# please see if AutoGen has released support for Ollama tools\n",
    "# if so, use the code written in previous section\n",
    "######################################################\n",
    "# copy this code into main.py and run it from terminal\n",
    "# to make sure file metadata funciton works properly\n",
    "# run main.py from the same folder where your path is\n",
    "# i.e. main.py and blutter_image.jpg are in same directory\n",
    "# else give abolute path in prompt\n",
    "######################################################\n",
    "\n",
    "\n",
    "# import autogen\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "import asyncio\n",
    "from autogen_core.tools import FunctionTool\n",
    "from autogen_core.models import ModelFamily\n",
    "from autogen_core.models import ChatCompletionClient\n",
    "\n",
    "import os\n",
    "import pwd\n",
    "import mimetypes\n",
    "import shutil\n",
    "\n",
    "import ollama\n",
    "\n",
    "def genLlamaResonse(prompt, context):\n",
    "    return ollama.chat(model=\"llama3.2\", \n",
    "            messages=[{'role': 'user', 'content': f'''I want to find out exact information.\n",
    "            Here is the question {prompt}.\n",
    "            \n",
    "            To answer this question, I have data available from a tool function from where I need to retreive anwser.\n",
    "            Here is the data: {context}\n",
    "            '''}],\n",
    ")\n",
    "\n",
    "file_path_processed = '/processed/dir'\n",
    "# Define a tool\n",
    "def get_FileMetaData(file_path: str) -> str:\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # Get file owner\n",
    "    file_owner = pwd.getpwuid(os.stat(file_path).st_uid).pw_name\n",
    "    # Get file type\n",
    "    file_type, _ = mimetypes.guess_type(file_path)\n",
    "\n",
    "    # not file metadata is read, move or rename this file as processed\n",
    "    # TODO: update this later as per business requirement\n",
    "    # shutil.move(file_path, file_path_processed)\n",
    "    \n",
    "    return f\"The file path is {file_path}, file owner is {file_owner}, file size is {file_size} and file type is {file_type}.\"\n",
    "\n",
    "\n",
    "# this code is not required for now\n",
    "# however once AutoGen release support for ollama\n",
    "# this will come handy\n",
    "# tools = FunctionTool(get_FileMetaData, description=\"Get the current file metadata\")\n",
    "\n",
    "custom_model_client = OpenAIChatCompletionClient(\n",
    "    model=\"deepseek-r1\",\n",
    "    base_url=\"http://0.0.0.0:4000\",\n",
    "    api_key=\"NotRequiredSinceWeAreLocal\",\n",
    "    model_info={\n",
    "        \"model\": \"deepseek-r1\",\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False,\n",
    "        \"family\": ModelFamily.R1,\n",
    "    },\n",
    ")\n",
    "\n",
    "from typing import Literal\n",
    "from typing_extensions import Annotated\n",
    "import autogen\n",
    "\n",
    "local_llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"NotRequired\",  # Loaded with LiteLLM command\n",
    "            \"api_key\": \"NotRequired\",  # Not needed\n",
    "            \"base_url\": \"http://0.0.0.0:4000\",  # Your LiteLLM URL\n",
    "            \"price\": [0, 0],  # Put in price per 1K tokens [prompt, response] as free!\n",
    "        }\n",
    "    ],\n",
    "    \"cache_seed\": None,  # Turns off caching, useful for testing different models\n",
    "}\n",
    "\n",
    "chatbot = autogen.AssistantAgent(\n",
    "    name=\"chatbot\",\n",
    "    system_message=\"\"\"For file management prompts,\n",
    "        only use the functions you have been provided with.\n",
    "        If the function has been called previously,\n",
    "        return only the word 'TERMINATE'.\"\"\",\n",
    "    llm_config=local_llm_config,\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and \"TERMINATE\" in x.get(\"content\", \"\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=1,\n",
    "    code_execution_config={\"work_dir\": \"code\", \"use_docker\": False},\n",
    ")\n",
    "\n",
    "# Register the function with the agent\n",
    "@user_proxy.register_for_execution()\n",
    "@chatbot.register_for_llm(description=\"file metadata function.\")\n",
    "def get_Metadata(\n",
    "    file_path: str\n",
    ") -> str:\n",
    "    return get_FileMetaData(file_path)\n",
    "\n",
    "prompt = \"find the file size of a file name blurred_image.jpg.\"\n",
    "async def main() -> None:\n",
    "# start the conversation\n",
    "    res = user_proxy.initiate_chat(\n",
    "        chatbot,\n",
    "        message=prompt,\n",
    "        summary_method=\"reflection_with_llm\",\n",
    "        # summary_method=\"last_msg\"\n",
    "    )\n",
    "    print(genLlamaResonse(prompt, chatbot.chat_messages))\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Build Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autogen\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# update main function to \n",
    "# add Team_Filer code\n",
    "\n",
    "async def main() -> None:\n",
    "    # Define an agent\n",
    "    Agent_Filer = AssistantAgent(\n",
    "                    name=\"Agent_Filer\",\n",
    "                    #     model_client=OpenAIChatCompletionClient(\n",
    "                    #     model=\"gpt-4o-2024-08-06\",\n",
    "                    #     # api_key=\"YOUR_API_KEY\",\n",
    "                    # ),\n",
    "                    # replace with ollama\n",
    "                    \n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    # Define a team with a single agent and maximum auto-gen turns of 1.\n",
    "    Team_Filer = RoundRobinGroupChat([Agent_Filer], max_turns=1)\n",
    "\n",
    "    while True:\n",
    "        # Get user input from the console.\n",
    "        user_input = input(\"Enter a message (type 'exit' to leave): \")\n",
    "        if user_input.strip().lower() == \"exit\":\n",
    "            break\n",
    "        # Run the team and stream messages to the console.\n",
    "        stream = Team_Filer.run_stream(task=user_input)\n",
    "        await Console(stream)\n",
    "\n",
    "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 6: Unit test\n",
    "\n",
    "while running Agent Studio, give a path to a local directory which has some PDF, txt and png files and see if it returns file metadata as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern: Team QC\n",
    "\n",
    "Now that we're familiar with writing models, tools, agents, and teams, we'll write the code again. This code is similar to Team Filer, but instead of one tool, we'll have three tool functions. Most other definitions will remain similar.\n",
    "\n",
    "Here is the complete code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autogen\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "#########################\n",
    "# TOOL_PDF\n",
    "#########################\n",
    "from pypdf import PdfReader\n",
    "\n",
    "async def get_PyPDF(file_path: str) -> str:\n",
    "    reader = PdfReader(file_path)\n",
    "    number_of_pages = len(reader.pages)\n",
    "    text = ''.join([page.extract_text() for page in reader.pages])\n",
    "    # print(text[:2155])\n",
    "    return text\n",
    "\n",
    "#########################\n",
    "# TOOL_Tesseract\n",
    "#########################\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "async def get_Tesseract(file_path: str) -> str:\n",
    "    text = pytesseract.image_to_string(Image.open(file_path))\n",
    "    return text\n",
    "\n",
    "#########################\n",
    "# TOOL_VisionAI\n",
    "#########################\n",
    "import requests\n",
    "async def get_VisionAI(file_path: str) -> str:\n",
    "    # Define the API endpoint and headers\n",
    "    api_endpoint = \"https://api.ollama.com/v1/llama3.3/vision/ocr\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer YOUR_API_KEY\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Read the file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        files = {'file': file}\n",
    "        \n",
    "        # Make the request to the API\n",
    "        response = requests.post(api_endpoint, headers=headers, files=files)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            ocr_result = response.json()\n",
    "            return ocr_result\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            return None\n",
    "\n",
    "## adding all tools to tools List\n",
    "tools = [{\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'get_PyPDF',\n",
    "        'description': 'Get the current content of a give PDF file',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'city': {\n",
    "              'type': 'string',\n",
    "              'description': 'File Path',\n",
    "            },\n",
    "          },\n",
    "          'required': ['file path'],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'get_Tesseract',\n",
    "        'description': 'Get the current content of a given image file',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'city': {\n",
    "              'type': 'string',\n",
    "              'description': 'File Path',\n",
    "            },\n",
    "          },\n",
    "          'required': ['file path'],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "  ]\n",
    "\n",
    "## define Team, Agent and run stream\n",
    "async def main() -> None:\n",
    "    # Define an agent\n",
    "    Agent_QC = AssistantAgent(\n",
    "                    name=\"Agent_QC\",\n",
    "                    #     model_client=OpenAIChatCompletionClient(\n",
    "                    #     model=\"gpt-4o-2024-08-06\",\n",
    "                    #     # api_key=\"YOUR_API_KEY\",\n",
    "                    # ),\n",
    "                    # replace with ollama\n",
    "                    \n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    # Define a team with a single agent and maximum auto-gen turns of 1.\n",
    "    Team_QC = RoundRobinGroupChat([Agent_QC], max_turns=1)\n",
    "\n",
    "    while True:\n",
    "        # Get user input from the console.\n",
    "        user_input = input(\"Enter a message (type 'exit' to leave): \")\n",
    "        if user_input.strip().lower() == \"exit\":\n",
    "            break\n",
    "        # Run the team and stream messages to the console.\n",
    "        stream = Team_QC.run_stream(task=user_input)\n",
    "        await Console(stream)\n",
    "\n",
    "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern: Team DB\n",
    "\n",
    "Now that we're familiar with writing models, tools, agents, and teams, we'll write the code again. This code is similar to Team Filer, but instead of one tool, we'll have two tool functions. Most other definitions will remain similar.\n",
    "\n",
    "Here is the complete code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autogen\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "#########################\n",
    "# TOOL_SQLDB\n",
    "#########################\n",
    "import sqlite3\n",
    "async def set_SQLDB(metadata: JSON) -> str:\n",
    "    # Connect to the database (or create it)\n",
    "    conn = sqlite3.connect('example.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create a table\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS fileRecords (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        type TEXT,\n",
    "        size FLOAT\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "    # Insert a record\n",
    "    cursor.execute('''\n",
    "    INSERT INTO fileRecords (name, type, size) VALUES (?, ?, ?)\n",
    "    ''', (metadata[\"file_name\"], metadata[\"file_type\"], metadata[\"file_size\"]))\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve records\n",
    "    cursor.execute('SELECT * FROM fileRecords')\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "from chromadb import ChromaDB\n",
    "#########################\n",
    "# TOOL_ChromaDB\n",
    "#########################\n",
    "\n",
    "def insert_data_into_chromadb(data, collection_name):\n",
    "    # Initialize ChromaDB client\n",
    "    client = ChromaDB()\n",
    "\n",
    "    # Create or get the collection\n",
    "    collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "    # Insert data into the collection\n",
    "    for item in data:\n",
    "        collection.insert(item)\n",
    "\n",
    "    print(f\"Data inserted into collection '{collection_name}' successfully.\")\n",
    "\n",
    "# Example usage\n",
    "data = [\n",
    "    {\"id\": 1, \"text\": \"Sample text 1\"},\n",
    "    {\"id\": 2, \"text\": \"Sample text 2\"},\n",
    "    {\"id\": 3, \"text\": \"Sample text 3\"}\n",
    "]\n",
    "collection_name = 'example_collection'\n",
    "insert_data_into_chromadb(data, collection_name)\n",
    "\n",
    "## adding all tools to tools List\n",
    "tools = [{\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'set_SQLDB',\n",
    "        'description': 'Get the current content of a give PDF file',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'city': {\n",
    "              'type': 'string',\n",
    "              'description': 'File Path',\n",
    "            },\n",
    "          },\n",
    "          'required': ['file path'],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'set_VectorDB',\n",
    "        'description': 'Get the current content of a given image file',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'city': {\n",
    "              'type': 'string',\n",
    "              'description': 'File Path',\n",
    "            },\n",
    "          },\n",
    "          'required': ['file path'],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "  ]\n",
    "\n",
    "## define Team, Agent and run stream\n",
    "async def main() -> None:\n",
    "    # Define an agent\n",
    "    Agent_DB = AssistantAgent(\n",
    "                    name=\"Agent_DB\",\n",
    "                    #     model_client=OpenAIChatCompletionClient(\n",
    "                    #     model=\"gpt-4o-2024-08-06\",\n",
    "                    #     # api_key=\"YOUR_API_KEY\",\n",
    "                    # ),\n",
    "                    # replace with ollama\n",
    "                    \n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    # Define a team with a single agent and maximum auto-gen turns of 1.\n",
    "    Team_DB = RoundRobinGroupChat([Agent_DB], max_turns=1)\n",
    "\n",
    "    while True:\n",
    "        # Get user input from the console.\n",
    "        user_input = input(\"Enter a message (type 'exit' to leave): \")\n",
    "        if user_input.strip().lower() == \"exit\":\n",
    "            break\n",
    "        # Run the team and stream messages to the console.\n",
    "        stream = Team_DB.run_stream(task=user_input)\n",
    "        await Console(stream)\n",
    "\n",
    "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human in Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magentic-One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Core"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
